{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00ea0e79-9e78-467d-a54d-6fcf18a10a37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import all libraries here\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af53209a-de58-4d9a-b1fd-514460e8616d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Part 1: Join the reddit Glossier Comments dataset sentiment+ the competitor comments Data set senemtiment with the Google trends data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c2637e-d4ed-42ee-9552-25b8e01df3a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+-------+----+-----+--------+\n",
       "|      Date|Sephora|Ulta|Fenty|Glossier|\n",
       "+----------+-------+----+-----+--------+\n",
       "|2021-01-01|     88| 100|   70|      35|\n",
       "|2021-01-02|     97|  96|  100|      41|\n",
       "|2021-01-03|    100|  96|   84|      44|\n",
       "|2021-01-04|     85|  74|   72|      32|\n",
       "|2021-01-05|     84|  70|   58|      35|\n",
       "+----------+-------+----+-----+--------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+-------+----+-----+--------+\n|      Date|Sephora|Ulta|Fenty|Glossier|\n+----------+-------+----+-----+--------+\n|2021-01-01|     88| 100|   70|      35|\n|2021-01-02|     97|  96|  100|      41|\n|2021-01-03|    100|  96|   84|      44|\n|2021-01-04|     85|  74|   72|      32|\n|2021-01-05|     84|  70|   58|      35|\n+----------+-------+----+-----+--------+\nonly showing top 5 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import the glossier comment dataset \n",
    "glos_comm = spark.read.parquet(\"/FileStore/glossier/glossier_comments\")\n",
    "#import the competitor comment data set \n",
    "competitor_comments = spark.read.parquet(\"dbfs:/FileStore/glossier/competitor_comments\")\n",
    "#import the google trends data set \n",
    "google = spark.read.csv('/FileStore/google_trends_competitors.csv', header='true')\n",
    "google.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d737c6e5-4b2b-492f-be0c-0774682bcd3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-------------------+\n",
       "|                body|        created_utc|\n",
       "+--------------------+-------------------+\n",
       "|i just got an ema...|2021-07-18 16:16:10|\n",
       "|milk kush is also...|2021-07-18 16:21:10|\n",
       "|i have dry skin a...|2021-07-18 16:23:52|\n",
       "|hi all  if you‚Äôd ...|2022-04-04 22:56:35|\n",
       "|glossier balm dot...|2022-04-04 22:58:38|\n",
       "+--------------------+-------------------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+-------------------+\n|                body|        created_utc|\n+--------------------+-------------------+\n|i just got an ema...|2021-07-18 16:16:10|\n|milk kush is also...|2021-07-18 16:21:10|\n|i have dry skin a...|2021-07-18 16:23:52|\n|hi all  if you‚Äôd ...|2022-04-04 22:56:35|\n|glossier balm dot...|2022-04-04 22:58:38|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data cleaning \n",
    "#import libraries\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace, translate\n",
    "import re \n",
    "#preliminary data cleaning in order to join appropriate colunms \n",
    "#Glossier \n",
    "#we know all we are interested in is tge body and the date \n",
    "glos_comm2 = glos_comm.select(\"body\",\"created_utc\")\n",
    "#change type \n",
    "glos_comm3 = glos_comm2.withColumn(\"created_utc\",glos_comm2.created_utc.cast('timestamp'))\n",
    "#glos_comm3.show(5)\n",
    "## converting all words to lowercase\n",
    "glos_comm4 = glos_comm3 .withColumn(\"body\",lower(translate('body', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~', ' ')))\n",
    "glos_comm4.show(5)\n",
    "#Now we can move on to doing actual sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0b01d1-38cc-4628-9537-c4a8aa2af8cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfhub_use download started this may take some time.\n",
       "Approximate size to download 923.7 MB\n",
       "\r",
       "[ | ]\r",
       "[OK!]\n",
       "sentimentdl_use_twitter download started this may take some time.\n",
       "Approximate size to download 11.4 MB\n",
       "\r",
       "[ | ]\r",
       "[OK!]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "tfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n\r[ | ]\r[OK!]\nsentimentdl_use_twitter download started this may take some time.\nApproximate size to download 11.4 MB\n\r[ | ]\r[OK!]\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now create the nlp pipeline\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "sentimentdl = SentimentDLModel.pretrained(name='sentimentdl_use_twitter', lang=\"en\")\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fd1aafe-665d-4079-87c5-6414ce555601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# running the pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipelineModel = nlpPipeline.fit(empty_df)\n",
    "\n",
    "data = glos_comm4.select(col(\"body\").alias(\"text\"))\n",
    "result= pipelineModel.transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a77e55ed-3eb1-4107-8237-3574ea09ec50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+---------+\n",
       "|                text|sentiment|\n",
       "+--------------------+---------+\n",
       "|i just got an ema...| negative|\n",
       "|milk kush is also...| positive|\n",
       "|i have dry skin a...| negative|\n",
       "|hi all  if you‚Äôd ...| positive|\n",
       "|glossier balm dot...| negative|\n",
       "+--------------------+---------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+---------+\n|                text|sentiment|\n+--------------------+---------+\n|i just got an ema...| negative|\n|milk kush is also...| positive|\n|i have dry skin a...| negative|\n|hi all  if you‚Äôd ...| positive|\n|glossier balm dot...| negative|\n+--------------------+---------+\nonly showing top 5 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "result = result.select('text', F.explode('sentiment.result').alias(\"sentiment\"))\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b52f97e-08d5-48c7-9480-bd1b959032d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-------------------+---------+\n",
       "|                body|        created_utc|sentiment|\n",
       "+--------------------+-------------------+---------+\n",
       "|\\n garnier micell...|2021-06-22 00:09:09|  neutral|\n",
       "|\\ndaniel sandler ...|2021-11-17 03:49:16| positive|\n",
       "|\\nso i love color...|2021-03-26 22:21:13| positive|\n",
       "|\\nthis post has b...|2022-08-30 21:54:39| negative|\n",
       "|\\r\\n\\nfinished of...|2022-06-07 21:04:19| positive|\n",
       "|  aw i was hoping...|2022-07-29 14:24:30| positive|\n",
       "| byredo bal dafri...|2021-04-09 03:32:46| positive|\n",
       "| first off both l...|2021-06-10 05:26:51|  neutral|\n",
       "| glossier you\\n a...|2022-01-17 15:12:34| positive|\n",
       "| i have the exact...|2022-02-13 23:42:41| positive|\n",
       "| i think people g...|2021-11-19 16:58:27|  neutral|\n",
       "| iso  shipping to...|2021-04-01 00:34:38| positive|\n",
       "| jane iredale spi...|2021-05-18 04:59:01| positive|\n",
       "| molecule 01  iri...|2022-07-28 14:48:01| positive|\n",
       "| murad invisiblur...|2022-06-30 18:47:41| negative|\n",
       "| pink hoodie  lov...|2021-11-28 23:09:49| positive|\n",
       "| red brown gen g ...|2021-03-26 17:24:35| positive|\n",
       "| step 1 moonshot ...|2021-01-01 07:25:16| positive|\n",
       "| that would be sa...|2021-06-22 23:00:36| negative|\n",
       "| you can try an a...|2021-02-21 09:37:06| positive|\n",
       "+--------------------+-------------------+---------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+-------------------+---------+\n|                body|        created_utc|sentiment|\n+--------------------+-------------------+---------+\n|\\n garnier micell...|2021-06-22 00:09:09|  neutral|\n|\\ndaniel sandler ...|2021-11-17 03:49:16| positive|\n|\\nso i love color...|2021-03-26 22:21:13| positive|\n|\\nthis post has b...|2022-08-30 21:54:39| negative|\n|\\r\\n\\nfinished of...|2022-06-07 21:04:19| positive|\n|  aw i was hoping...|2022-07-29 14:24:30| positive|\n| byredo bal dafri...|2021-04-09 03:32:46| positive|\n| first off both l...|2021-06-10 05:26:51|  neutral|\n| glossier you\\n a...|2022-01-17 15:12:34| positive|\n| i have the exact...|2022-02-13 23:42:41| positive|\n| i think people g...|2021-11-19 16:58:27|  neutral|\n| iso  shipping to...|2021-04-01 00:34:38| positive|\n| jane iredale spi...|2021-05-18 04:59:01| positive|\n| molecule 01  iri...|2022-07-28 14:48:01| positive|\n| murad invisiblur...|2022-06-30 18:47:41| negative|\n| pink hoodie  lov...|2021-11-28 23:09:49| positive|\n| red brown gen g ...|2021-03-26 17:24:35| positive|\n| step 1 moonshot ...|2021-01-01 07:25:16| positive|\n| that would be sa...|2021-06-22 23:00:36| negative|\n| you can try an a...|2021-02-21 09:37:06| positive|\n+--------------------+-------------------+---------+\nonly showing top 20 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we need to join the gloss_com4 data set with our result \n",
    "#we are going to join on body \n",
    "#so reamke column in result \n",
    "result.createOrReplaceTempView(\"result_vw\")\n",
    "glos_comm4.createOrReplaceTempView(\"glossier_vw\")\n",
    "glossier_final= spark.sql(\"select glossier_vw.*, result_vw.sentiment \\\n",
    "                    from glossier_vw join result_vw on glossier_vw.body = result_vw.text\")\n",
    "glossier_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cba5fbec-519c-4e90-946f-d9f8a85f19ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-------------------+---------+\n",
       "|                body|        created_utc|sentiment|\n",
       "+--------------------+-------------------+---------+\n",
       "|\\n garnier micell...|2021-06-22 00:09:09|        1|\n",
       "|\\ndaniel sandler ...|2021-11-17 03:49:16|        2|\n",
       "|\\nso i love color...|2021-03-26 22:21:13|        2|\n",
       "|\\nthis post has b...|2022-08-30 21:54:39|        0|\n",
       "|\\r\\n\\nfinished of...|2022-06-07 21:04:19|        2|\n",
       "|  aw i was hoping...|2022-07-29 14:24:30|        2|\n",
       "| byredo bal dafri...|2021-04-09 03:32:46|        2|\n",
       "| first off both l...|2021-06-10 05:26:51|        1|\n",
       "| glossier you\\n a...|2022-01-17 15:12:34|        2|\n",
       "| i have the exact...|2022-02-13 23:42:41|        2|\n",
       "| i think people g...|2021-11-19 16:58:27|        1|\n",
       "| iso  shipping to...|2021-04-01 00:34:38|        2|\n",
       "| jane iredale spi...|2021-05-18 04:59:01|        2|\n",
       "| molecule 01  iri...|2022-07-28 14:48:01|        2|\n",
       "| murad invisiblur...|2022-06-30 18:47:41|        0|\n",
       "| pink hoodie  lov...|2021-11-28 23:09:49|        2|\n",
       "| red brown gen g ...|2021-03-26 17:24:35|        2|\n",
       "| step 1 moonshot ...|2021-01-01 07:25:16|        2|\n",
       "| that would be sa...|2021-06-22 23:00:36|        0|\n",
       "| you can try an a...|2021-02-21 09:37:06|        2|\n",
       "+--------------------+-------------------+---------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+-------------------+---------+\n|                body|        created_utc|sentiment|\n+--------------------+-------------------+---------+\n|\\n garnier micell...|2021-06-22 00:09:09|        1|\n|\\ndaniel sandler ...|2021-11-17 03:49:16|        2|\n|\\nso i love color...|2021-03-26 22:21:13|        2|\n|\\nthis post has b...|2022-08-30 21:54:39|        0|\n|\\r\\n\\nfinished of...|2022-06-07 21:04:19|        2|\n|  aw i was hoping...|2022-07-29 14:24:30|        2|\n| byredo bal dafri...|2021-04-09 03:32:46|        2|\n| first off both l...|2021-06-10 05:26:51|        1|\n| glossier you\\n a...|2022-01-17 15:12:34|        2|\n| i have the exact...|2022-02-13 23:42:41|        2|\n| i think people g...|2021-11-19 16:58:27|        1|\n| iso  shipping to...|2021-04-01 00:34:38|        2|\n| jane iredale spi...|2021-05-18 04:59:01|        2|\n| molecule 01  iri...|2022-07-28 14:48:01|        2|\n| murad invisiblur...|2022-06-30 18:47:41|        0|\n| pink hoodie  lov...|2021-11-28 23:09:49|        2|\n| red brown gen g ...|2021-03-26 17:24:35|        2|\n| step 1 moonshot ...|2021-01-01 07:25:16|        2|\n| that would be sa...|2021-06-22 23:00:36|        0|\n| you can try an a...|2021-02-21 09:37:06|        2|\n+--------------------+-------------------+---------+\nonly showing top 20 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now I am going to make the sentiment into a dummy variables \n",
    "#2= positive \n",
    "#1= neutral \n",
    "#0=negative\n",
    "glossier_final2=glossier_final.na.replace(\"negative\",\"0\")\n",
    "glossier_final3=glossier_final2.na.replace(\"neutral\",\"1\")\n",
    "glossier_final4=glossier_final3.na.replace(\"positive\",\"2\")\n",
    "#change sentiment type to integer\n",
    "glossier_final5=glossier_final4.withColumn(\"sentiment\",col(\"sentiment\").cast(\"int\"))\n",
    "glossier_final5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e403357-0440-45f1-ac1b-7f4027c5a022",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+--------------------+\n",
       "|       day|      avg(sentiment)|\n",
       "+----------+--------------------+\n",
       "|2021-11-03|  1.7736081152490102|\n",
       "|2021-12-23|   1.852250885179565|\n",
       "|2022-05-17|  1.8446363000180848|\n",
       "|2021-04-06|  1.5618166013450658|\n",
       "|2022-03-30|  1.5942850910677282|\n",
       "|2021-11-15|  1.5524269174022856|\n",
       "|2021-10-25|  1.7197809726068958|\n",
       "|2021-02-18|0.030630739487675206|\n",
       "|2021-08-30|  1.3847968297899107|\n",
       "|2021-09-23|   1.537916250197275|\n",
       "|2022-07-04|  1.6347265221878224|\n",
       "|2022-07-08|  1.5617837093528228|\n",
       "|2022-01-20|    1.77389811104751|\n",
       "|2021-01-15| 0.05284008779913272|\n",
       "|2022-07-30|  1.6973674037420752|\n",
       "|2021-02-13|  1.6691449814126393|\n",
       "|2022-07-23|  1.5777883328295694|\n",
       "|2022-03-21|  1.6067963833237144|\n",
       "|2021-10-27|  1.8363393309077694|\n",
       "|2021-08-06|  1.5152900986280236|\n",
       "+----------+--------------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+--------------------+\n|       day|      avg(sentiment)|\n+----------+--------------------+\n|2021-11-03|  1.7736081152490102|\n|2021-12-23|   1.852250885179565|\n|2022-05-17|  1.8446363000180848|\n|2021-04-06|  1.5618166013450658|\n|2022-03-30|  1.5942850910677282|\n|2021-11-15|  1.5524269174022856|\n|2021-10-25|  1.7197809726068958|\n|2021-02-18|0.030630739487675206|\n|2021-08-30|  1.3847968297899107|\n|2021-09-23|   1.537916250197275|\n|2022-07-04|  1.6347265221878224|\n|2022-07-08|  1.5617837093528228|\n|2022-01-20|    1.77389811104751|\n|2021-01-15| 0.05284008779913272|\n|2022-07-30|  1.6973674037420752|\n|2021-02-13|  1.6691449814126393|\n|2022-07-23|  1.5777883328295694|\n|2022-03-21|  1.6067963833237144|\n|2021-10-27|  1.8363393309077694|\n|2021-08-06|  1.5152900986280236|\n+----------+--------------------+\nonly showing top 20 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now the last step before the join would be to aggraget based on the day \n",
    "#for the sentiment column I am going to take the avg for the day\n",
    "from pyspark.sql import functions as F\n",
    "glossier_final6=glossier_final5.select(F.date_format('created_utc','yyyy-MM-dd').alias('day'),'sentiment').groupby('day').mean('sentiment')\n",
    "glossier_final6.show()\n",
    "#now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd49fd09-8c42-41a2-8428-fafbdbb00db1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>avg(sentiment)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>1.773608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>1.852251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-17</td>\n",
       "      <td>1.844636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>1.561817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-25</td>\n",
       "      <td>1.719781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-11-15</td>\n",
       "      <td>1.552427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-03-30</td>\n",
       "      <td>1.594285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>1.384797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>0.030631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>1.537916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-07-08</td>\n",
       "      <td>1.561784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-01-20</td>\n",
       "      <td>1.773898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-07-04</td>\n",
       "      <td>1.634727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-07-30</td>\n",
       "      <td>1.697367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>0.052840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-07-23</td>\n",
       "      <td>1.577788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>1.606796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-02-13</td>\n",
       "      <td>1.669145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>1.836339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-08-06</td>\n",
       "      <td>1.515290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>avg(sentiment)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-11-03</td>\n      <td>1.773608</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-12-23</td>\n      <td>1.852251</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-05-17</td>\n      <td>1.844636</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-04-06</td>\n      <td>1.561817</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-10-25</td>\n      <td>1.719781</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2021-11-15</td>\n      <td>1.552427</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2022-03-30</td>\n      <td>1.594285</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2021-08-30</td>\n      <td>1.384797</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2021-02-18</td>\n      <td>0.030631</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2021-09-23</td>\n      <td>1.537916</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2022-07-08</td>\n      <td>1.561784</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2022-01-20</td>\n      <td>1.773898</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2022-07-04</td>\n      <td>1.634727</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2022-07-30</td>\n      <td>1.697367</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2021-01-15</td>\n      <td>0.052840</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2022-07-23</td>\n      <td>1.577788</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2022-03-21</td>\n      <td>1.606796</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2021-02-13</td>\n      <td>1.669145</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2021-10-27</td>\n      <td>1.836339</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2021-08-06</td>\n      <td>1.515290</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we turn this into a pnadas dataframe \n",
    "glossier_pd_final = glossier_final6.toPandas()\n",
    "glossier_pd_final.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf69558-7f05-4ed6-959e-443adb7f58b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-----------+-------+-----+-----+--------+\n",
       "|                body|created_utc|Sephora| Ulta|Fenty|Glossier|\n",
       "+--------------------+-----------+-------+-----+-----+--------+\n",
       "|This is the nices...| 1650684062|  false|false|false|   false|\n",
       "|        Thank you üòä| 1650684086|  false|false|false|   false|\n",
       "|Thank youü•∫y‚Äôall ...| 1650684158|  false|false|false|   false|\n",
       "|i think both look...| 1650684185|  false|false|false|   false|\n",
       "|Hah never heard t...| 1650684185|  false|false|false|   false|\n",
       "|You look gorgeous...| 1650684310|  false|false|false|   false|\n",
       "|Have you tried Il...| 1650684313|  false|false|false|   false|\n",
       "|Robert Welsh has ...| 1650684341|  false|false|false|   false|\n",
       "|Have you consider...| 1650684416|  false|false|false|   false|\n",
       "|I love the blonde...| 1650684455|  false|false|false|   false|\n",
       "|               Thick| 1650684620|  false|false|false|   false|\n",
       "|***Thank you for ...| 1650684734|  false|false|false|   false|\n",
       "|You can rock both...| 1650684766|  false|false|false|   false|\n",
       "|Absolutely love t...| 1650684827|  false|false|false|   false|\n",
       "|This is amazing!!...| 1650684835|  false|false|false|   false|\n",
       "|           [deleted]| 1650684848|  false|false|false|   false|\n",
       "|I think your bare...| 1650684872|  false|false|false|   false|\n",
       "|           Spot. On.| 1650685127|  false|false|false|   false|\n",
       "|Unreal! You're ta...| 1650685160|  false|false|false|   false|\n",
       "|Ohhhh, I swoon wi...| 1649553570|  false|false|false|   false|\n",
       "|Thank you so much...| 1649553583|  false|false|false|   false|\n",
       "|I see that was yo...| 1649553623|  false|false|false|   false|\n",
       "|Color theory is a...| 1649553628|  false|false|false|   false|\n",
       "|It‚Äôs the end of t...| 1649553732|  false|false|false|   false|\n",
       "|I think bronze nu...| 1649553752|  false|false|false|   false|\n",
       "|That‚Äôs so kind th...| 1649553753|  false|false|false|   false|\n",
       "|Thank you so much...| 1649553799|  false|false|false|   false|\n",
       "|It‚Äôs funny becaus...| 1649553806|  false|false|false|   false|\n",
       "|Thank you so much...| 1649553867|  false|false|false|   false|\n",
       "|       Thank you!!‚ò∫Ô∏è| 1649553944|  false|false|false|   false|\n",
       "|     Thank you!!üòÅüíö| 1649553964|  false|false|false|   false|\n",
       "|Anything by Cover...| 1649553970|  false|false|false|   false|\n",
       "|       Thank ya!‚ò∫Ô∏èüíö| 1649553975|  false|false|false|   false|\n",
       "|u look so pretty ...| 1649553999|  false|false|false|   false|\n",
       "|2nd! I‚Äôd love to ...| 1649554048|  false|false|false|   false|\n",
       "|I would say thank...| 1649554124|  false|false|false|   false|\n",
       "|https://youtu.be/...| 1649554136|  false|false|false|   false|\n",
       "|Thanks! I did tak...| 1649554163|  false|false|false|   false|\n",
       "|Thank you so so m...| 1649554206|  false|false|false|   false|\n",
       "|Thank you!!! Okay...| 1649554232|  false|false|false|   false|\n",
       "+--------------------+-----------+-------+-----+-----+--------+\n",
       "only showing top 40 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+-----------+-------+-----+-----+--------+\n|                body|created_utc|Sephora| Ulta|Fenty|Glossier|\n+--------------------+-----------+-------+-----+-----+--------+\n|This is the nices...| 1650684062|  false|false|false|   false|\n|        Thank you üòä| 1650684086|  false|false|false|   false|\n|Thank youü•∫y‚Äôall ...| 1650684158|  false|false|false|   false|\n|i think both look...| 1650684185|  false|false|false|   false|\n|Hah never heard t...| 1650684185|  false|false|false|   false|\n|You look gorgeous...| 1650684310|  false|false|false|   false|\n|Have you tried Il...| 1650684313|  false|false|false|   false|\n|Robert Welsh has ...| 1650684341|  false|false|false|   false|\n|Have you consider...| 1650684416|  false|false|false|   false|\n|I love the blonde...| 1650684455|  false|false|false|   false|\n|               Thick| 1650684620|  false|false|false|   false|\n|***Thank you for ...| 1650684734|  false|false|false|   false|\n|You can rock both...| 1650684766|  false|false|false|   false|\n|Absolutely love t...| 1650684827|  false|false|false|   false|\n|This is amazing!!...| 1650684835|  false|false|false|   false|\n|           [deleted]| 1650684848|  false|false|false|   false|\n|I think your bare...| 1650684872|  false|false|false|   false|\n|           Spot. On.| 1650685127|  false|false|false|   false|\n|Unreal! You're ta...| 1650685160|  false|false|false|   false|\n|Ohhhh, I swoon wi...| 1649553570|  false|false|false|   false|\n|Thank you so much...| 1649553583|  false|false|false|   false|\n|I see that was yo...| 1649553623|  false|false|false|   false|\n|Color theory is a...| 1649553628|  false|false|false|   false|\n|It‚Äôs the end of t...| 1649553732|  false|false|false|   false|\n|I think bronze nu...| 1649553752|  false|false|false|   false|\n|That‚Äôs so kind th...| 1649553753|  false|false|false|   false|\n|Thank you so much...| 1649553799|  false|false|false|   false|\n|It‚Äôs funny becaus...| 1649553806|  false|false|false|   false|\n|Thank you so much...| 1649553867|  false|false|false|   false|\n|       Thank you!!‚ò∫Ô∏è| 1649553944|  false|false|false|   false|\n|     Thank you!!üòÅüíö| 1649553964|  false|false|false|   false|\n|Anything by Cover...| 1649553970|  false|false|false|   false|\n|       Thank ya!‚ò∫Ô∏èüíö| 1649553975|  false|false|false|   false|\n|u look so pretty ...| 1649553999|  false|false|false|   false|\n|2nd! I‚Äôd love to ...| 1649554048|  false|false|false|   false|\n|I would say thank...| 1649554124|  false|false|false|   false|\n|https://youtu.be/...| 1649554136|  false|false|false|   false|\n|Thanks! I did tak...| 1649554163|  false|false|false|   false|\n|Thank you so so m...| 1649554206|  false|false|false|   false|\n|Thank you!!! Okay...| 1649554232|  false|false|false|   false|\n+--------------------+-----------+-------+-----+-----+--------+\nonly showing top 40 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we have to do the same process on the competitors comments ! \n",
    "#A lot of this code is taken from Q4_final (1)\n",
    "#As a result I have already checked for null values ect \n",
    "competitor_list = [\"Makeup\", \"MakeupAddiction\"]\n",
    "competitor_comments2 = competitor_comments.filter(competitor_comments.subreddit.isin(competitor_list))\n",
    "competitor_comments3=competitor_comments2.select(\"body\",\"created_utc\")\n",
    "#now we need to filter based on Sephora, Ulta, Fenty and Glossier\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "competitor_comments4=competitor_comments3.withColumn(\"Sephora\",col(\"body\").rlike(\"Sephora|sephora\"))\n",
    "competitor_comments5=competitor_comments4.withColumn(\"Ulta\",col(\"body\").rlike(\"Ulta|ulta\"))\n",
    "competitor_comments6=competitor_comments5.withColumn(\"Fenty\",col(\"body\").rlike(\"Fenty|fenty\"))\n",
    "competitor_comments7=competitor_comments6.withColumn(\"Glossier\",col(\"body\").rlike(\"Glossier|glossier\"))\n",
    "competitor_comments7.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e7a45c7-b66e-4bfa-8789-db5ace7ca4b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-----------+-------+\n",
       "|                body|created_utc|Sephora|\n",
       "+--------------------+-----------+-------+\n",
       "|Honestly that‚Äôs w...| 1626234041|Sephora|\n",
       "|My advice is to g...| 1649364278|Sephora|\n",
       "|I'm not an expert...| 1618713621|Sephora|\n",
       "|Hey that's incred...| 1660949791|Sephora|\n",
       "|Tell them you pur...| 1660949878|Sephora|\n",
       "|I can't help but ...| 1660949894|Sephora|\n",
       "|I'm really sad th...| 1660949978|Sephora|\n",
       "|Sephora brand is ...| 1647907798|Sephora|\n",
       "|Buy Freck! It‚Äôs u...| 1648093418|Sephora|\n",
       "|[Auric Glow Lust ...| 1622834012|Sephora|\n",
       "|I‚Äôve used this: h...| 1650582232|Sephora|\n",
       "|I know ‚òπÔ∏è unfortu...| 1649333002|Sephora|\n",
       "|40s. Everything b...| 1647096398|Sephora|\n",
       "|I can‚Äôt speak to ...| 1648615892|Sephora|\n",
       "|Face:\\nElf porele...| 1652492053|Sephora|\n",
       "|Some of the other...| 1661046675|Sephora|\n",
       "|Wording, I meant ...| 1628515971|Sephora|\n",
       "|I use to work in ...| 1649245020|Sephora|\n",
       "|I only got the $5...| 1649245275|Sephora|\n",
       "|Brow: Anastasia D...| 1648780656|Sephora|\n",
       "+--------------------+-----------+-------+\n",
       "only showing top 20 rows\n",
       "\n",
       "+--------------------+-----------+----+\n",
       "|                body|created_utc|Ulta|\n",
       "+--------------------+-----------+----+\n",
       "|It could be a com...| 1644974190|Ulta|\n",
       "|Honestly that‚Äôs w...| 1626234041|Ulta|\n",
       "|Can confirm that ...| 1643776124|Ulta|\n",
       "|Thank you for the...| 1650502820|Ulta|\n",
       "|Maybelline Fit Me...| 1661136083|Ulta|\n",
       "|Face: \\n\\ncoverfx...| 1649032724|Ulta|\n",
       "|A few years ago i...| 1660949690|Ulta|\n",
       "|Tell them you pur...| 1660949878|Ulta|\n",
       "|It‚Äôs at my Ulta !...| 1648099247|Ulta|\n",
       "|The ulta app had ...| 1648831407|Ulta|\n",
       "|Fenty skincare is...| 1648093597|Ulta|\n",
       "|My favorite favor...| 1650581523|Ulta|\n",
       "|This for sure! Go...| 1650582241|Ulta|\n",
       "|I know ‚òπÔ∏è unfortu...| 1649333002|Ulta|\n",
       "|FYI- Ulta is goin...| 1646688311|Ulta|\n",
       "|It was directly t...| 1649204511|Ulta|\n",
       "|I can‚Äôt speak to ...| 1648615892|Ulta|\n",
       "|Not to mention th...| 1627411336|Ulta|\n",
       "|Wording, I meant ...| 1628515971|Ulta|\n",
       "|I like to go to u...| 1653962478|Ulta|\n",
       "+--------------------+-----------+----+\n",
       "only showing top 20 rows\n",
       "\n",
       "+--------------------+-----------+-----+\n",
       "|                body|created_utc|Fenty|\n",
       "+--------------------+-----------+-----+\n",
       "|It depends on whi...| 1626234169|Fenty|\n",
       "|‚ú®PRODUCT BREAKDOW...| 1641184283|Fenty|\n",
       "|Products used:\\n\\...| 1642789647|Fenty|\n",
       "|full product list...| 1649364012|Fenty|\n",
       "|It depends on wha...| 1649364457|Fenty|\n",
       "|SAME! I really wa...| 1615949872|Fenty|\n",
       "|Fenty Beauty make...| 1639858798|Fenty|\n",
       "|Glycolic acid ton...| 1660950248|Fenty|\n",
       "|Fenty skincare is...| 1648093597|Fenty|\n",
       "|I love the Fenty ...| 1646687995|Fenty|\n",
       "|Face:\\nElf porele...| 1652492053|Fenty|\n",
       "|maybelline fit me...| 1629262920|Fenty|\n",
       "|Brow: Anastasia D...| 1648780656|Fenty|\n",
       "|PRODUCTS:\\n\\nBROW...| 1653257011|Fenty|\n",
       "|I love the fenty ...| 1653257339|Fenty|\n",
       "|Foundation: Charl...| 1624120988|Fenty|\n",
       "|I wanted to give ...| 1644206231|Fenty|\n",
       "|‚è∫ Wheel Picks Ran...| 1637031538|Fenty|\n",
       "|Maybe the milani ...| 1651123107|Fenty|\n",
       "|Ib: @ naezrahlook...| 1656640628|Fenty|\n",
       "+--------------------+-----------+-----+\n",
       "only showing top 20 rows\n",
       "\n",
       "+--------------------+-----------+--------+\n",
       "|                body|created_utc|Glossier|\n",
       "+--------------------+-----------+--------+\n",
       "|if you haven‚Äôt al...| 1649364432|Glossier|\n",
       "|A brown mascara, ...| 1637541842|Glossier|\n",
       "|Tell them you pur...| 1660949878|Glossier|\n",
       "|Products used:\\n\\...| 1648099359|Glossier|\n",
       "|[Auric Glow Lust ...| 1622834012|Glossier|\n",
       "|cant go wrong wit...| 1649204998|Glossier|\n",
       "|Okay update like ...| 1652492416|Glossier|\n",
       "|Some of the other...| 1661046675|Glossier|\n",
       "|Ib: @ naezrahlook...| 1656640628|Glossier|\n",
       "|I think the Pat M...| 1636416432|Glossier|\n",
       "|Product list:\\nEy...| 1642719462|Glossier|\n",
       "|Makeup details:\\n...| 1643856732|Glossier|\n",
       "| Glossier lash slick| 1650250874|Glossier|\n",
       "|Maybe Glossier's ...| 1648400528|Glossier|\n",
       "|I think the solut...| 1648869164|Glossier|\n",
       "|I use Tarte Shape...| 1621505973|Glossier|\n",
       "|My skincare routi...| 1646276848|Glossier|\n",
       "| Glossier Haloscope!| 1641304286|Glossier|\n",
       "|Oh my gosh as soo...| 1640269562|Glossier|\n",
       "|I have the same i...| 1649658045|Glossier|\n",
       "+--------------------+-----------+--------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+-----------+-------+\n|                body|created_utc|Sephora|\n+--------------------+-----------+-------+\n|Honestly that‚Äôs w...| 1626234041|Sephora|\n|My advice is to g...| 1649364278|Sephora|\n|I'm not an expert...| 1618713621|Sephora|\n|Hey that's incred...| 1660949791|Sephora|\n|Tell them you pur...| 1660949878|Sephora|\n|I can't help but ...| 1660949894|Sephora|\n|I'm really sad th...| 1660949978|Sephora|\n|Sephora brand is ...| 1647907798|Sephora|\n|Buy Freck! It‚Äôs u...| 1648093418|Sephora|\n|[Auric Glow Lust ...| 1622834012|Sephora|\n|I‚Äôve used this: h...| 1650582232|Sephora|\n|I know ‚òπÔ∏è unfortu...| 1649333002|Sephora|\n|40s. Everything b...| 1647096398|Sephora|\n|I can‚Äôt speak to ...| 1648615892|Sephora|\n|Face:\\nElf porele...| 1652492053|Sephora|\n|Some of the other...| 1661046675|Sephora|\n|Wording, I meant ...| 1628515971|Sephora|\n|I use to work in ...| 1649245020|Sephora|\n|I only got the $5...| 1649245275|Sephora|\n|Brow: Anastasia D...| 1648780656|Sephora|\n+--------------------+-----------+-------+\nonly showing top 20 rows\n\n+--------------------+-----------+----+\n|                body|created_utc|Ulta|\n+--------------------+-----------+----+\n|It could be a com...| 1644974190|Ulta|\n|Honestly that‚Äôs w...| 1626234041|Ulta|\n|Can confirm that ...| 1643776124|Ulta|\n|Thank you for the...| 1650502820|Ulta|\n|Maybelline Fit Me...| 1661136083|Ulta|\n|Face: \\n\\ncoverfx...| 1649032724|Ulta|\n|A few years ago i...| 1660949690|Ulta|\n|Tell them you pur...| 1660949878|Ulta|\n|It‚Äôs at my Ulta !...| 1648099247|Ulta|\n|The ulta app had ...| 1648831407|Ulta|\n|Fenty skincare is...| 1648093597|Ulta|\n|My favorite favor...| 1650581523|Ulta|\n|This for sure! Go...| 1650582241|Ulta|\n|I know ‚òπÔ∏è unfortu...| 1649333002|Ulta|\n|FYI- Ulta is goin...| 1646688311|Ulta|\n|It was directly t...| 1649204511|Ulta|\n|I can‚Äôt speak to ...| 1648615892|Ulta|\n|Not to mention th...| 1627411336|Ulta|\n|Wording, I meant ...| 1628515971|Ulta|\n|I like to go to u...| 1653962478|Ulta|\n+--------------------+-----------+----+\nonly showing top 20 rows\n\n+--------------------+-----------+-----+\n|                body|created_utc|Fenty|\n+--------------------+-----------+-----+\n|It depends on whi...| 1626234169|Fenty|\n|‚ú®PRODUCT BREAKDOW...| 1641184283|Fenty|\n|Products used:\\n\\...| 1642789647|Fenty|\n|full product list...| 1649364012|Fenty|\n|It depends on wha...| 1649364457|Fenty|\n|SAME! I really wa...| 1615949872|Fenty|\n|Fenty Beauty make...| 1639858798|Fenty|\n|Glycolic acid ton...| 1660950248|Fenty|\n|Fenty skincare is...| 1648093597|Fenty|\n|I love the Fenty ...| 1646687995|Fenty|\n|Face:\\nElf porele...| 1652492053|Fenty|\n|maybelline fit me...| 1629262920|Fenty|\n|Brow: Anastasia D...| 1648780656|Fenty|\n|PRODUCTS:\\n\\nBROW...| 1653257011|Fenty|\n|I love the fenty ...| 1653257339|Fenty|\n|Foundation: Charl...| 1624120988|Fenty|\n|I wanted to give ...| 1644206231|Fenty|\n|‚è∫ Wheel Picks Ran...| 1637031538|Fenty|\n|Maybe the milani ...| 1651123107|Fenty|\n|Ib: @ naezrahlook...| 1656640628|Fenty|\n+--------------------+-----------+-----+\nonly showing top 20 rows\n\n+--------------------+-----------+--------+\n|                body|created_utc|Glossier|\n+--------------------+-----------+--------+\n|if you haven‚Äôt al...| 1649364432|Glossier|\n|A brown mascara, ...| 1637541842|Glossier|\n|Tell them you pur...| 1660949878|Glossier|\n|Products used:\\n\\...| 1648099359|Glossier|\n|[Auric Glow Lust ...| 1622834012|Glossier|\n|cant go wrong wit...| 1649204998|Glossier|\n|Okay update like ...| 1652492416|Glossier|\n|Some of the other...| 1661046675|Glossier|\n|Ib: @ naezrahlook...| 1656640628|Glossier|\n|I think the Pat M...| 1636416432|Glossier|\n|Product list:\\nEy...| 1642719462|Glossier|\n|Makeup details:\\n...| 1643856732|Glossier|\n| Glossier lash slick| 1650250874|Glossier|\n|Maybe Glossier's ...| 1648400528|Glossier|\n|I think the solut...| 1648869164|Glossier|\n|I use Tarte Shape...| 1621505973|Glossier|\n|My skincare routi...| 1646276848|Glossier|\n| Glossier Haloscope!| 1641304286|Glossier|\n|Oh my gosh as soo...| 1640269562|Glossier|\n|I have the same i...| 1649658045|Glossier|\n+--------------------+-----------+--------+\nonly showing top 20 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we need to fiter based on whether is says true \n",
    "competitor_comments_Sephora=competitor_comments7.filter(\"Sephora==True\")\n",
    "#Now lets replace the True with Sephora \n",
    "competitor_comments_Sephora2=competitor_comments_Sephora.withColumn(\"Sephora\",col(\"Sephora\").cast(\"string\"))\n",
    "competitor_comments_Sephora3=competitor_comments_Sephora2.na.replace(\"true\",\"Sephora\")\n",
    "competitor_comments_Sephora4=competitor_comments_Sephora3.select(\"body\",\"created_utc\",\"Sephora\")\n",
    "competitor_comments_Sephora4.show(20)\n",
    "competitor_comments_Ulta=competitor_comments7.filter(\"Ulta==True\")\n",
    "competitor_comments_Ulta2=competitor_comments_Ulta.withColumn(\"Ulta\",col(\"Ulta\").cast(\"string\"))\n",
    "competitor_comments_Ulta3=competitor_comments_Ulta2.na.replace(\"true\",\"Ulta\")\n",
    "competitor_comments_Ulta4=competitor_comments_Ulta3.select(\"body\",\"created_utc\",\"Ulta\")\n",
    "competitor_comments_Ulta4.show(20)\n",
    "competitor_comments_Fenty=competitor_comments7.filter(\"Fenty==True\")\n",
    "competitor_comments_Fenty2=competitor_comments_Fenty.withColumn(\"Fenty\",col(\"Fenty\").cast(\"string\"))\n",
    "competitor_comments_Fenty3=competitor_comments_Fenty2.na.replace(\"true\",\"Fenty\")\n",
    "competitor_comments_Fenty4=competitor_comments_Fenty3.select(\"body\",\"created_utc\",\"Fenty\")\n",
    "competitor_comments_Fenty4.show(20)\n",
    "competitor_comments_Glossier=competitor_comments7.filter(\"Glossier==True\")\n",
    "competitor_comments_Glossier2=competitor_comments_Glossier.withColumn(\"Glossier\",col(\"Glossier\").cast(\"string\"))\n",
    "competitor_comments_Glossier3=competitor_comments_Glossier2.na.replace(\"true\",\"Glossier\")\n",
    "competitor_comments_Glossier4=competitor_comments_Glossier3.select(\"body\",\"created_utc\",\"Glossier\")\n",
    "competitor_comments_Glossier4.show(20)\n",
    "#Now lets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b7e2d8d-956d-40db-bca6-000f66eacbdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-------------------+-------+\n",
       "|                body|        created_utc|  Brand|\n",
       "+--------------------+-------------------+-------+\n",
       "|honestly that‚Äôs w...|2021-07-14 03:40:41|Sephora|\n",
       "|my advice is to g...|2022-04-07 20:44:38|Sephora|\n",
       "|im not an expert ...|2021-04-18 02:40:21|Sephora|\n",
       "|hey thats incredi...|2022-08-19 22:56:31|Sephora|\n",
       "|tell them you pur...|2022-08-19 22:57:58|Sephora|\n",
       "+--------------------+-------------------+-------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+-------------------+-------+\n|                body|        created_utc|  Brand|\n+--------------------+-------------------+-------+\n|honestly that‚Äôs w...|2021-07-14 03:40:41|Sephora|\n|my advice is to g...|2022-04-07 20:44:38|Sephora|\n|im not an expert ...|2021-04-18 02:40:21|Sephora|\n|hey thats incredi...|2022-08-19 22:56:31|Sephora|\n|tell them you pur...|2022-08-19 22:57:58|Sephora|\n+--------------------+-------------------+-------+\nonly showing top 5 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Okay now I need to stack these dataframes \n",
    "#let me rename the colums so they all match \n",
    "competitor_comments_Sephora5=competitor_comments_Sephora4.withColumnRenamed(\"Sephora\",\"Brand\")\n",
    "competitor_comments_Ulta5=competitor_comments_Ulta4.withColumnRenamed(\"Ulta\",\"Brand\")\n",
    "competitor_comments_Fenty5=competitor_comments_Fenty4.withColumnRenamed(\"Fenty\",\"Brand\")\n",
    "competitor_comments_Glossier5=competitor_comments_Glossier4.withColumnRenamed(\"Glossier\",\"Brand\")\n",
    "#Now I need to stack the dataframes \n",
    "competitor_temp= competitor_comments_Sephora5.union(competitor_comments_Ulta5)\n",
    "competitor_temp2= competitor_temp.union(competitor_comments_Fenty5)\n",
    "competitor_temp3= competitor_temp2.union(competitor_comments_Glossier5)\n",
    "#one more thing change timestamp and make eveythign in the body lower \n",
    "competitor_temp4= competitor_temp3.withColumn(\"created_utc\",competitor_temp3.created_utc.cast('timestamp'))\n",
    "competitor_temp5= competitor_temp4.withColumn(\"body\",lower(translate('body', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~', ' ')))\n",
    "competitor_temp5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c79dd4-79d4-487a-b3c2-66ac5feeed45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfhub_use download started this may take some time.\n",
       "Approximate size to download 923.7 MB\n",
       "\r",
       "[ | ]\r",
       "[OK!]\n",
       "sentimentdl_use_twitter download started this may take some time.\n",
       "Approximate size to download 11.4 MB\n",
       "\r",
       "[ | ]\r",
       "[OK!]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "tfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n\r[ | ]\r[OK!]\nsentimentdl_use_twitter download started this may take some time.\nApproximate size to download 11.4 MB\n\r[ | ]\r[OK!]\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we can run the sentiment model \n",
    "#Now create the nlp pipeline\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "sentimentdl = SentimentDLModel.pretrained(name='sentimentdl_use_twitter', lang=\"en\")\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd4c58bb-42d6-43a5-b1b8-c64d09d35520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# running the pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "pipelineModel = nlpPipeline.fit(empty_df)\n",
    "\n",
    "data = competitor_temp5.select(col(\"body\").alias(\"text\"))\n",
    "result= pipelineModel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c75f6fb1-7fd7-4b52-a2af-53de28da0052",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+---------+\n",
       "|                text|sentiment|\n",
       "+--------------------+---------+\n",
       "|honestly that‚Äôs w...| negative|\n",
       "|my advice is to g...| positive|\n",
       "|im not an expert ...| positive|\n",
       "|hey thats incredi...| positive|\n",
       "|tell them you pur...| positive|\n",
       "+--------------------+---------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+---------+\n|                text|sentiment|\n+--------------------+---------+\n|honestly that‚Äôs w...| negative|\n|my advice is to g...| positive|\n|im not an expert ...| positive|\n|hey thats incredi...| positive|\n|tell them you pur...| positive|\n+--------------------+---------+\nonly showing top 5 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "result = result.select('text', F.explode('sentiment.result').alias(\"sentiment\"))\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "366d5bcf-8c2c-4fb3-91d1-a8e56844852d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-------------------+--------+---------+\n",
       "|                body|        created_utc|   Brand|sentiment|\n",
       "+--------------------+-------------------+--------+---------+\n",
       "|for long term mis...|2022-05-13 08:20:57| Sephora| positive|\n",
       "|base colourpop no...|2022-04-05 23:57:04| Sephora| positive|\n",
       "|i get rewards wit...|2022-02-12 22:39:45| Sephora| positive|\n",
       "|correct me if im ...|2021-12-20 00:37:18| Sephora| positive|\n",
       "|correct me if im ...|2021-12-20 00:37:18|    Ulta| positive|\n",
       "|hey there \\n\\n\\nd...|2022-03-21 11:51:54| Sephora| positive|\n",
       "|thanks i‚Äôve been ...|2021-04-04 20:38:33| Sephora| positive|\n",
       "|i stopped doing w...|2022-07-23 07:07:28| Sephora| negative|\n",
       "|i have and love t...|2022-05-13 02:42:26| Sephora| positive|\n",
       "|scrape wipe same ...|2021-10-02 15:22:09| Sephora| positive|\n",
       "|scrape wipe same ...|2021-10-02 15:22:09|    Ulta| positive|\n",
       "|i work for sephor...|2022-08-17 04:26:58| Sephora| positive|\n",
       "|sephora had a tin...|2021-08-30 19:51:16| Sephora| negative|\n",
       "|‚Äú100 pure‚Äù brand ...|2021-06-17 07:43:16| Sephora|  neutral|\n",
       "|sephora employees...|2022-08-17 04:28:02| Sephora| positive|\n",
       "|alamar cosmetics‚Äô...|2022-06-06 04:16:20| Sephora| positive|\n",
       "|try a sample at s...|2021-08-19 08:55:55| Sephora| positive|\n",
       "|‚ú¥ top 5 favorites...|2021-11-06 04:29:49| Sephora| negative|\n",
       "|‚ú¥ top 5 favorites...|2021-11-06 04:29:49|Glossier| negative|\n",
       "|try looking up so...|2022-06-25 02:14:41| Sephora| positive|\n",
       "+--------------------+-------------------+--------+---------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+-------------------+--------+---------+\n|                body|        created_utc|   Brand|sentiment|\n+--------------------+-------------------+--------+---------+\n|for long term mis...|2022-05-13 08:20:57| Sephora| positive|\n|base colourpop no...|2022-04-05 23:57:04| Sephora| positive|\n|i get rewards wit...|2022-02-12 22:39:45| Sephora| positive|\n|correct me if im ...|2021-12-20 00:37:18| Sephora| positive|\n|correct me if im ...|2021-12-20 00:37:18|    Ulta| positive|\n|hey there \\n\\n\\nd...|2022-03-21 11:51:54| Sephora| positive|\n|thanks i‚Äôve been ...|2021-04-04 20:38:33| Sephora| positive|\n|i stopped doing w...|2022-07-23 07:07:28| Sephora| negative|\n|i have and love t...|2022-05-13 02:42:26| Sephora| positive|\n|scrape wipe same ...|2021-10-02 15:22:09| Sephora| positive|\n|scrape wipe same ...|2021-10-02 15:22:09|    Ulta| positive|\n|i work for sephor...|2022-08-17 04:26:58| Sephora| positive|\n|sephora had a tin...|2021-08-30 19:51:16| Sephora| negative|\n|‚Äú100 pure‚Äù brand ...|2021-06-17 07:43:16| Sephora|  neutral|\n|sephora employees...|2022-08-17 04:28:02| Sephora| positive|\n|alamar cosmetics‚Äô...|2022-06-06 04:16:20| Sephora| positive|\n|try a sample at s...|2021-08-19 08:55:55| Sephora| positive|\n|‚ú¥ top 5 favorites...|2021-11-06 04:29:49| Sephora| negative|\n|‚ú¥ top 5 favorites...|2021-11-06 04:29:49|Glossier| negative|\n|try looking up so...|2022-06-25 02:14:41| Sephora| positive|\n+--------------------+-------------------+--------+---------+\nonly showing top 20 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we need to join the gloss_com4 data set with our result \n",
    "#we are going to join on body \n",
    "#so reamke column in result \n",
    "result.createOrReplaceTempView(\"result_vw\")\n",
    "competitor_temp5.createOrReplaceTempView(\"competitor_vw\")\n",
    "competitor_final= spark.sql(\"select competitor_vw.*, result_vw.sentiment \\\n",
    "                    from competitor_vw join result_vw on competitor_vw.body = result_vw.text\")\n",
    "competitor_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94ed1c6a-b9a4-4594-a4fd-91b6ab5382e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-------------------+--------+---------+\n",
       "|                body|        created_utc|   Brand|sentiment|\n",
       "+--------------------+-------------------+--------+---------+\n",
       "|for long term mis...|2022-05-13 08:20:57| Sephora|        2|\n",
       "|base colourpop no...|2022-04-05 23:57:04| Sephora|        2|\n",
       "|correct me if im ...|2021-12-20 00:37:18|    Ulta|        2|\n",
       "|correct me if im ...|2021-12-20 00:37:18| Sephora|        2|\n",
       "|thanks i‚Äôve been ...|2021-04-04 20:38:33| Sephora|        2|\n",
       "|alamar cosmetics‚Äô...|2022-06-06 04:16:20| Sephora|        2|\n",
       "|try a sample at s...|2021-08-19 08:55:55| Sephora|        2|\n",
       "|‚ú¥ top 5 favorites...|2021-11-06 04:29:49|Glossier|        0|\n",
       "|‚ú¥ top 5 favorites...|2021-11-06 04:29:49| Sephora|        0|\n",
       "|try looking up so...|2022-06-25 02:14:41| Sephora|        2|\n",
       "|you are the consu...|2021-11-28 03:06:37|    Ulta|        2|\n",
       "|you are the consu...|2021-11-28 03:06:37| Sephora|        2|\n",
       "|used modern renai...|2021-03-13 01:40:39| Sephora|        2|\n",
       "|product list\\n\\nf...|2021-02-18 20:36:30| Sephora|        2|\n",
       "|ok  i spoke to on...|2022-02-11 23:22:19| Sephora|        0|\n",
       "|im not an expert ...|2021-04-18 02:40:21| Sephora|        2|\n",
       "|ohhh that is such...|2022-01-03 03:57:42|    Ulta|        2|\n",
       "|ohhh that is such...|2022-01-03 03:57:42| Sephora|        2|\n",
       "|i put on mine usi...|2022-05-15 01:36:20| Sephora|        0|\n",
       "|hey op nars soft ...|2022-03-31 02:18:38| Sephora|        2|\n",
       "+--------------------+-------------------+--------+---------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+--------------------+-------------------+--------+---------+\n|                body|        created_utc|   Brand|sentiment|\n+--------------------+-------------------+--------+---------+\n|for long term mis...|2022-05-13 08:20:57| Sephora|        2|\n|base colourpop no...|2022-04-05 23:57:04| Sephora|        2|\n|correct me if im ...|2021-12-20 00:37:18|    Ulta|        2|\n|correct me if im ...|2021-12-20 00:37:18| Sephora|        2|\n|thanks i‚Äôve been ...|2021-04-04 20:38:33| Sephora|        2|\n|alamar cosmetics‚Äô...|2022-06-06 04:16:20| Sephora|        2|\n|try a sample at s...|2021-08-19 08:55:55| Sephora|        2|\n|‚ú¥ top 5 favorites...|2021-11-06 04:29:49|Glossier|        0|\n|‚ú¥ top 5 favorites...|2021-11-06 04:29:49| Sephora|        0|\n|try looking up so...|2022-06-25 02:14:41| Sephora|        2|\n|you are the consu...|2021-11-28 03:06:37|    Ulta|        2|\n|you are the consu...|2021-11-28 03:06:37| Sephora|        2|\n|used modern renai...|2021-03-13 01:40:39| Sephora|        2|\n|product list\\n\\nf...|2021-02-18 20:36:30| Sephora|        2|\n|ok  i spoke to on...|2022-02-11 23:22:19| Sephora|        0|\n|im not an expert ...|2021-04-18 02:40:21| Sephora|        2|\n|ohhh that is such...|2022-01-03 03:57:42|    Ulta|        2|\n|ohhh that is such...|2022-01-03 03:57:42| Sephora|        2|\n|i put on mine usi...|2022-05-15 01:36:20| Sephora|        0|\n|hey op nars soft ...|2022-03-31 02:18:38| Sephora|        2|\n+--------------------+-------------------+--------+---------+\nonly showing top 20 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now I am going to make the sentiment into a dummy variables \n",
    "#2= positive \n",
    "#1= neutral \n",
    "#0=negative\n",
    "competitor_final2=competitor_final.na.replace(\"negative\",\"0\")\n",
    "competitor_final3=competitor_final2.na.replace(\"neutral\",\"1\")\n",
    "competitor_final4=competitor_final3.na.replace(\"positive\",\"2\")\n",
    "#change sentiment type to integer\n",
    "competitor_final5=competitor_final4.withColumn(\"sentiment\",col(\"sentiment\").cast(\"int\"))\n",
    "competitor_final5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c3472e-c947-4457-b59d-1b322ca381c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------+------------------+\n",
       "|       day|    avg(sentiment)|\n",
       "+----------+------------------+\n",
       "|2021-05 22|1.2857142857142858|\n",
       "|2021-02 03|               1.4|\n",
       "|2022-04 07|               1.6|\n",
       "|2021-05 07|               1.5|\n",
       "|2021-05 26|               1.4|\n",
       "+----------+------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "+----------+------------------+\n",
       "|       day|    avg(sentiment)|\n",
       "+----------+------------------+\n",
       "|2021-01 22|0.8888888888888888|\n",
       "|2021-11 22|1.4666666666666666|\n",
       "|2021-04 29|1.7777777777777777|\n",
       "|2021-02 28|0.8888888888888888|\n",
       "|2022-04 30|             1.875|\n",
       "+----------+------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "+----------+------------------+\n",
       "|       day|    avg(sentiment)|\n",
       "+----------+------------------+\n",
       "|2022-07 19|1.7777777777777777|\n",
       "|2021-04 16|1.8666666666666667|\n",
       "|2021-03 14|               2.0|\n",
       "|2021-12 13|              1.84|\n",
       "|2021-01 02|              1.45|\n",
       "+----------+------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "+----------+------------------+\n",
       "|       day|    avg(sentiment)|\n",
       "+----------+------------------+\n",
       "|2021-10 06|1.7272727272727273|\n",
       "|2021-04 01|1.5294117647058822|\n",
       "|2022-02 08|1.7692307692307692|\n",
       "|2021-05 19|1.7333333333333334|\n",
       "|2021-08 12|               1.0|\n",
       "+----------+------------------+\n",
       "only showing top 5 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----------+------------------+\n|       day|    avg(sentiment)|\n+----------+------------------+\n|2021-05 22|1.2857142857142858|\n|2021-02 03|               1.4|\n|2022-04 07|               1.6|\n|2021-05 07|               1.5|\n|2021-05 26|               1.4|\n+----------+------------------+\nonly showing top 5 rows\n\n+----------+------------------+\n|       day|    avg(sentiment)|\n+----------+------------------+\n|2021-01 22|0.8888888888888888|\n|2021-11 22|1.4666666666666666|\n|2021-04 29|1.7777777777777777|\n|2021-02 28|0.8888888888888888|\n|2022-04 30|             1.875|\n+----------+------------------+\nonly showing top 5 rows\n\n+----------+------------------+\n|       day|    avg(sentiment)|\n+----------+------------------+\n|2022-07 19|1.7777777777777777|\n|2021-04 16|1.8666666666666667|\n|2021-03 14|               2.0|\n|2021-12 13|              1.84|\n|2021-01 02|              1.45|\n+----------+------------------+\nonly showing top 5 rows\n\n+----------+------------------+\n|       day|    avg(sentiment)|\n+----------+------------------+\n|2021-10 06|1.7272727272727273|\n|2021-04 01|1.5294117647058822|\n|2022-02 08|1.7692307692307692|\n|2021-05 19|1.7333333333333334|\n|2021-08 12|               1.0|\n+----------+------------------+\nonly showing top 5 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Okay now I need to aggregate by day but since I also have brand I need to take that into account\n",
    "#filter by brand first \n",
    "c_Sephora=competitor_final5.filter(\"Brand=='Sephora'\")\n",
    "c_Ulta=competitor_final5.filter(\"Brand=='Ulta'\")\n",
    "c_Fenty=competitor_final5.filter(\"Brand=='Fenty'\")\n",
    "c_Glossier=competitor_final5.filter(\"Brand=='Glossier'\")\n",
    "#Now get score\n",
    "c_Sephora2=c_Sephora.select(F.date_format('created_utc','yyyy-MM dd').alias('day'),'sentiment').groupby('day').mean('sentiment')\n",
    "c_Sephora2.show(5)\n",
    "c_Ulta2=c_Ulta.select(F.date_format('created_utc','yyyy-MM dd').alias('day'),'sentiment').groupby('day').mean('sentiment')\n",
    "c_Ulta2.show(5)\n",
    "c_Fenty2=c_Fenty.select(F.date_format('created_utc','yyyy-MM dd').alias('day'),'sentiment').groupby('day').mean('sentiment')\n",
    "c_Fenty2.show(5)\n",
    "c_Glossier2=c_Glossier.select(F.date_format('created_utc','yyyy-MM dd').alias('day'),'sentiment').groupby('day').mean('sentiment')\n",
    "c_Glossier2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a53589-7d95-4503-bf01-473062d050fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>avg(sentiment)</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05 02</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Glossier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04 02</td>\n",
       "      <td>1.692308</td>\n",
       "      <td>Glossier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06 20</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>Glossier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10 06</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>Glossier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07 02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>Glossier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>avg(sentiment)</th>\n      <th>Brand</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-05 02</td>\n      <td>0.750000</td>\n      <td>Glossier</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-04 02</td>\n      <td>1.692308</td>\n      <td>Glossier</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-06 20</td>\n      <td>2.000000</td>\n      <td>Glossier</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-10 06</td>\n      <td>1.727273</td>\n      <td>Glossier</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-07 02</td>\n      <td>2.000000</td>\n      <td>Glossier</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make them all into pandas dataframes and add a column with the brand name \n",
    "#source: https://stackoverflow.com/questions/24039023/add-column-with-constant-value-to-pandas-dataframe\n",
    "Sephora_c_pd = c_Sephora2.toPandas()\n",
    "Sephora_c_pd['Brand']='Sephora'\n",
    "Sephora_c_pd.head(5)\n",
    "Ulta_c_pd = c_Ulta2.toPandas()\n",
    "Ulta_c_pd['Brand']='Ulta'\n",
    "Ulta_c_pd.head(5)\n",
    "Fenty_c_pd = c_Fenty2.toPandas()\n",
    "Fenty_c_pd['Brand']='Fenty'\n",
    "Fenty_c_pd.head(5)\n",
    "Glossier_c_pd = c_Glossier2.toPandas()\n",
    "Glossier_c_pd['Brand']='Glossier'\n",
    "Glossier_c_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091a75a4-66b4-4ee2-bf87-ffa34bdb6aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>avg(sentiment)</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-08 30</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05 15</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02 03</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10 14</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04 07</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-05 26</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-07 06</td>\n",
       "      <td>1.684211</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-01 26</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-06 01</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-04 30</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-04 20</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-04 17</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-01 25</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-08 20</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-03 15</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-02 23</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-08 20</td>\n",
       "      <td>1.088889</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-02 27</td>\n",
       "      <td>1.095238</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-02 22</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12 24</td>\n",
       "      <td>1.551724</td>\n",
       "      <td>Sephora</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>avg(sentiment)</th>\n      <th>Brand</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-08 30</td>\n      <td>0.666667</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-05 15</td>\n      <td>1.454545</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-02 03</td>\n      <td>1.400000</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-10 14</td>\n      <td>1.150000</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-04 07</td>\n      <td>1.600000</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2021-05 26</td>\n      <td>1.400000</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2022-07 06</td>\n      <td>1.684211</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2022-01 26</td>\n      <td>1.900000</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2022-06 01</td>\n      <td>1.428571</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2022-04 30</td>\n      <td>1.531250</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2022-04 20</td>\n      <td>1.333333</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2021-04 17</td>\n      <td>0.750000</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2022-01 25</td>\n      <td>1.350000</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2021-08 20</td>\n      <td>1.437500</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2022-03 15</td>\n      <td>1.384615</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2021-02 23</td>\n      <td>1.066667</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2022-08 20</td>\n      <td>1.088889</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2022-02 27</td>\n      <td>1.095238</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2021-02 22</td>\n      <td>1.333333</td>\n      <td>Sephora</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2021-12 24</td>\n      <td>1.551724</td>\n      <td>Sephora</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now stack these dataframes \n",
    "import pandas as pd\n",
    "comp_final_pd=pd.concat([Sephora_c_pd, Ulta_c_pd], ignore_index=True, axis=0)\n",
    "#comp_final_pd.count()\n",
    "comp_final_pd2=pd.concat([comp_final_pd, Fenty_c_pd], ignore_index=True, axis=0)\n",
    "#comp_final_pd2.count()\n",
    "comp_final_pd3=pd.concat([comp_final_pd2, Glossier_c_pd], ignore_index=True, axis=0)\n",
    "#comp_final_pd3.count()\n",
    "comp_final_pd3.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9318816-3419-4d8e-8822-775f6e7b87c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          Date  average_sentiment Glossier\n",
       "509 2021-01-01           0.013614       35\n",
       "428 2021-01-02           0.118040       41\n",
       "300 2021-01-03           0.066365       44\n",
       "482 2021-01-04           0.023517       32\n",
       "437 2021-01-05           0.036824       35\n",
       "..         ...                ...      ...\n",
       "352 2022-08-27           1.459990       43\n",
       "473 2022-08-28           1.835665       38\n",
       "540 2022-08-29           1.486830       41\n",
       "180 2022-08-30           1.626132       36\n",
       "329 2022-08-31           1.903914       33\n",
       "\n",
       "[608 rows x 3 columns]\n",
       "           Date  average_sentiment     Brand Sephora Ulta Fenty Glossier\n",
       "935  2021-01-01           1.500000   Sephora      88  100    70       35\n",
       "936  2021-01-01           1.454545      Ulta      88  100    70       35\n",
       "937  2021-01-01           1.826087     Fenty      88  100    70       35\n",
       "938  2021-01-01           1.300000  Glossier      88  100    70       35\n",
       "1842 2021-01-02           1.700000   Sephora      97   96   100       41\n",
       "...         ...                ...       ...     ...  ...   ...      ...\n",
       "1349 2022-08-30           1.272727   Sephora      95   65    33       36\n",
       "562  2022-08-31           0.846154   Sephora      88   67    30       33\n",
       "565  2022-08-31           1.600000  Glossier      88   67    30       33\n",
       "564  2022-08-31           1.636364     Fenty      88   67    30       33\n",
       "563  2022-08-31           0.666667      Ulta      88   67    30       33\n",
       "\n",
       "[2412 rows x 7 columns]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "          Date  average_sentiment Glossier\n509 2021-01-01           0.013614       35\n428 2021-01-02           0.118040       41\n300 2021-01-03           0.066365       44\n482 2021-01-04           0.023517       32\n437 2021-01-05           0.036824       35\n..         ...                ...      ...\n352 2022-08-27           1.459990       43\n473 2022-08-28           1.835665       38\n540 2022-08-29           1.486830       41\n180 2022-08-30           1.626132       36\n329 2022-08-31           1.903914       33\n\n[608 rows x 3 columns]\n           Date  average_sentiment     Brand Sephora Ulta Fenty Glossier\n935  2021-01-01           1.500000   Sephora      88  100    70       35\n936  2021-01-01           1.454545      Ulta      88  100    70       35\n937  2021-01-01           1.826087     Fenty      88  100    70       35\n938  2021-01-01           1.300000  Glossier      88  100    70       35\n1842 2021-01-02           1.700000   Sephora      97   96   100       41\n...         ...                ...       ...     ...  ...   ...      ...\n1349 2022-08-30           1.272727   Sephora      95   65    33       36\n562  2022-08-31           0.846154   Sephora      88   67    30       33\n565  2022-08-31           1.600000  Glossier      88   67    30       33\n564  2022-08-31           1.636364     Fenty      88   67    30       33\n563  2022-08-31           0.666667      Ulta      88   67    30       33\n\n[2412 rows x 7 columns]\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Okay now all we have left to do is join on the day with the external data \n",
    "glossier_pd_final.rename(columns={'day': 'Date'}, inplace=True)\n",
    "#print(glossier_pd.dtypes)\n",
    "#make to datetime object so its consistent \n",
    "glossier_pd_final['Date']=pd.to_datetime(glossier_pd_final['Date'])\n",
    "#print(glossier_pd_final.head(20))\n",
    "comp_final_pd3.rename(columns={'day': 'Date'}, inplace=True)\n",
    "#print(comp_final_pd3.dtypes)\n",
    "comp_final_pd3['Date']=pd.to_datetime(comp_final_pd3['Date'])\n",
    "#print(comp_final_pd3.head(20))\n",
    "#google = google.toPandas()\n",
    "#print(google_pd.dtypes)\n",
    "google_pd['Date']=pd.to_datetime(google_pd['Date'])\n",
    "#print(google_pd.head(20))\n",
    "#Now we can do our join \n",
    "final_glossier=glossier_pd_final.merge(google_pd, on='Date')\n",
    "#Since this is just for Glossier, delete columns we dont want such as Sephora, Ulta and Fenty \n",
    "final_glossier2=final_glossier.drop(columns=['Sephora', 'Ulta','Fenty'])\n",
    "final_glossier2.rename(columns={'avg(sentiment)': 'average_sentiment'}, inplace=True)\n",
    "#Now sort by date \n",
    "final_glossier3=final_glossier2.sort_values(by='Date')\n",
    "print(final_glossier3)\n",
    "#Now do the competitor data \n",
    "final_competitor=comp_final_pd3.merge(google_pd, on='Date')\n",
    "final_competitor.rename(columns={'avg(sentiment)': 'average_sentiment'}, inplace=True)\n",
    "#Now sort by date \n",
    "final_competitor2=final_competitor.sort_values(by='Date')\n",
    "print(final_competitor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f5f212f-9f65-44ea-82c9-b0cb4e733752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_glossier3.to_csv(\"/dbfs/FileStore/google_glossier.csv\")\n",
    "final_competitor2.to_csv(\"/dbfs/FileStore/google_competitors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a1e5ea4-d18d-4166-b181-6f6d123272c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "google_trends_reddit",
   "notebookOrigID": 94105572018528,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
