{"cells":[{"cell_type":"markdown","source":["Business Goal 2: We will identify which products should be in the newest Glossier kit."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03f73080-a529-4d2e-b646-698855206073","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Technical Proposal: We will scrape the Glossier website to identify its complete set of products. We will then use NLP techniques to identify which posts and comments in the Glossier subreddit contain these products. We will conduct sentiment analysis of each post to assign positive, negative, or neutral values. We will sum the number of posts and comments by product for positive posts. We will identify the top 10 products with the highest activity (positive sentiment). Similarly, we will sum the number of posts and comments by product for negative posts. We will identify the top 10 products with the highest activity (negative sentiment). We will display this information on a faceted chart to show executive audiences which products should be promoted, and which should potentially be discontinued."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfb04dd6-997e-42f6-9e3e-2da77d4517c2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pip install nltk"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f3ccfdf-a249-48ce-bc1f-4552c858db86","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Read in the data \nglos_comm = spark.read.parquet(\"/FileStore/glossier/glossier_comments\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69f49cb3-c6e7-4919-874c-42ca49366290","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Remove all uneccessary columns from the data frame \ncols = (\"author_cakeday\",\"author_flair_css_class\",\"author_flair_text\",\"permalink\",\"stickied\",\"gilded\",\"distinguished\",\"can_gild\",\"retrieved_on\",\"edited\")\nglos_comm = glos_comm.drop(*cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b39be06-acf1-47eb-88ae-907ab3123409","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Convert the columns to appropriate data type \nglos_comm = glos_comm.withColumn(\"created_utc\",glos_comm.created_utc.cast('timestamp'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"874e881c-323d-441d-ad68-d42b98b19c83","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Save the final data frame \nglos_comm_final = glos_comm"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7225882c-7953-4d44-8636-0f47d4c19b1a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Read in the data \nglos_sub = spark.read.parquet(\"/FileStore/glossier/glossier_submissions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f9509bb-09a9-44ee-815f-052b6faf99eb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Drop all of the uneccessary columns\ncols = (\"whitelist_status\",\"url\",\"thumbnail_width\",\"thumbnail_height\",\"thumbnail\",\"third_party_tracking_2\",\"third_party_tracking\",\"third_party_trackers\",\"suggested_sort\",\n       \"secure_media_embed\", \"retrieved_on\", \"promoted_url\", \"parent_whitelist_status\", \"link_flair_text\", \"link_flair_css_class\", \"imp_pixel\", \"href_url\", \"gilded\", \"embed_url\", \n       \"author_flair_css_class\", \"author_cakeday\",\"adserver_imp_pixel\", \"adserver_click_url\", \"secure_media_embed\", \"secure_media\", \"post_hint\", \"permalink\", \"original_link\", \n       \"mobile_ad_url\", \"embed_type\", \"domain_override\", \"domain\", \"author\", \"preview\", \"author_flair_text\", \"edited\", \"crosspost_parent_list\", \"media\", \"media_embed\")\nglos_sub = glos_sub.drop(*cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe5a6d1a-b7c6-461b-85a3-1261b03c4471","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Convert the columns to appropriate data type \nglos_sub_final = glos_sub.withColumn(\"created_utc\",glos_sub.created_utc.cast('timestamp'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02383cc6-45c0-479f-807f-fbf781ae3dfd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Get a combination of the submissions and comments data frame \ntop_prods = glos_comm_final.select(\"body\")\nnew = glos_sub_final.select(\"title\").alias(\"body\")\ntop_prods = top_prods.union(new)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2216cb1-55d8-4183-92a2-e435dd23ab4c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Website Used: https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb\n## Define a pipeline for text data cleaning \n\n## Assemble the document \nfrom sparknlp.base import DocumentAssembler\ndocumentAssembler = DocumentAssembler() \\\n     .setInputCol(\"body\") \\\n     .setOutputCol('document')\n\n## 1. Tokenize the data \nfrom sparknlp.annotator import Tokenizer\ntokenizer = Tokenizer() \\\n     .setInputCols(['document']) \\\n     .setOutputCol('tokenized')\n\n## 2. Normalize (or clean the data / remove characters)\nfrom sparknlp.annotator import Normalizer\nnormalizer = Normalizer() \\\n     .setInputCols(['tokenized']) \\\n     .setOutputCol('normalized') \\\n     .setLowercase(True)\n\n## 3. Lemmatize the data \nfrom sparknlp.annotator import LemmatizerModel\nlemmatizer = LemmatizerModel.pretrained() \\\n     .setInputCols(['normalized']) \\\n     .setOutputCol('lemmatized')\n\n## 4. Remove stopwords \nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\neng_stopwords = stopwords.words('english')\nfrom sparknlp.annotator import StopWordsCleaner\nstopwords_cleaner = StopWordsCleaner() \\\n     .setInputCols(['lemmatized']) \\\n     .setOutputCol('unigrams') \\\n     .setStopWords(eng_stopwords)\n\n## 5. Transform the data further with finisher\nfrom sparknlp.base import Finisher\nfinisher = Finisher() \\\n     .setInputCols(['unigrams'])\n\n## Develop the pipeline \nfrom pyspark.ml import Pipeline\npipeline = Pipeline() \\\n     .setStages([documentAssembler,                  \n                 tokenizer,\n                 normalizer,                  \n                 lemmatizer,                  \n                 stopwords_cleaner,  \n                 finisher])\n\n## Transform the text \nprocessed_review = pipeline.fit(top_prods).transform(top_prods)\n\n## 6. Then, concatenate the data in order to run sentiment analysis \nfrom pyspark.sql.functions import col, concat_ws\ndf2 = processed_review.withColumn(\"finished_unigrams\", concat_ws(\" \",col(\"finished_unigrams\")))\ndf2.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0bf205ce-d09d-438a-a8ea-8778c0fb6cbf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Get the text distributions after cleaning\nimport pyspark.sql.functions as F\ndf_temp = processed_review.withColumn('text_dist', F.size('finished_unigrams'))\ndf_temp.describe(\"text_dist\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86a67807-000c-4538-8beb-34c8319503e3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Convert the data frame into a view to run SQL on \ndf2.createOrReplaceTempView(\"df2_vw\")\n\n## Determine if a post or comment contains the respective product or not \nproduct_df = spark.sql(\"select body, finished_unigrams, \\\n                    case when lower(finished_unigrams) like '%bdc%' then 1 else 0 end as bdc, \\\n                    case when lower(finished_unigrams) like '%lashslick%' then 1 else 0 end as lashslick, \\\n                    case when lower(finished_unigrams) like '%powder%' then 1 else 0 end as powder, \\\n                    case when lower(finished_unigrams) like '%concealer%' then 1 else 0 end as concealer, \\\n                    case when lower(finished_unigrams) like '%mascara%' then 1 else 0 end as mascara,\\\n                    case when lower(finished_unigrams) like '%highlighter%' then 1 else 0 end as highlighter, \\\n                    case when lower(finished_unigrams) like '%candle%' then 1 else 0 end as candle, \\\n                    case when lower(finished_unigrams) like '%rollerball%' then 1 else 0 end as rollerball,\\\n                    case when lower(finished_unigrams) like '%balm_dot_com%' or lower(finished_unigrams) like '%balm_dotcom%' then 1 else 0 end as balm_dot_com, \\\n                    case when lower(finished_unigrams) like '%hoodie%' then 1 else 0 end as hoodie, \\\n                    case when lower(finished_unigrams) like '%tote%' then 1 else 0 end as tote, \\\n                    case when lower(finished_unigrams) like '%boybrow%' then 1 else 0 end as boybrow, \\\n                    case when lower(finished_unigrams) like '%brow gel%' then 1 else 0 end as brow_gel, \\\n                    case when lower(finished_unigrams) like '%generation g%' then 1 else 0 end as generation_g, \\\n                    case when lower(finished_unigrams) like '%scarf%' then 1 else 0 end as scarf, \\\n                    case when lower(finished_unigrams) like '%clip%' or lower(finished_unigrams) like '%clips%' then 1 else 0 end as clip, \\\n                    case when lower(finished_unigrams) like '%hair clips%' then 1 else 0 end as hair_clips, \\\n                    case when lower(finished_unigrams) like '%perfume%' then 1 else 0 end as perfume, \\\n                    case when lower(finished_unigrams) like '%cloud paint%' or lower(finished_unigrams) like '%cloudpaint%' then 1 else 0 end as cloud_paint, \\\n                    case when lower(finished_unigrams) like '%perfume%'  then 1 else 0 end as blush, \\\n                    case when lower(finished_unigrams) like '%eyeliner%' then 1 else 0 end as eyeliner, \\\n                    case when lower(finished_unigrams) like '%no. 1 pencil%' or lower(finished_unigrams) like '%no.1 pencil%' then 1 else 0 end as pencil, \\\n                    case when lower(finished_unigrams) like '%skin tint%' then 1 else 0 end as skin_tint, \\\n                    case when lower(finished_unigrams) like '%stretch concealer%' then 1 else 0 end as stretch_concealer, \\\n                    case when lower(finished_unigrams) like '%pomade%' then 1 else 0 end as pomade, \\\n                    case when lower(finished_unigrams) like '%lipstick%' then 1 else 0 end as lipstick, \\\n                    case when lower(finished_unigrams) like '%lash slick%' then 1 else 0 end as lash_slick, \\\n                    case when lower(finished_unigrams) like '%brush%' then 1 else 0 end as brush, \\\n                    case when lower(finished_unigrams) like '%eyeshadow%' then 1 else 0 end as eyeshadow, \\\n                    case when lower(finished_unigrams) like '%pro tip%' or lower(finished_unigrams) like '%protip%' then 1 else 0 end as protip, \\\n                    case when lower(finished_unigrams) like '%solar paint%' or lower(finished_unigrams) like '%solarpaint%' then 1 else 0 end as solar_paint, \\\n                    case when lower(finished_unigrams) like '%bronzer%' then 1 else 0 end as bronzer, \\\n                    case when lower(finished_unigrams) like '%monochromes%' then 1 else 0 end as monochromes, \\\n                    case when lower(finished_unigrams) like '%milky oil%' then 1 else 0 end as milky_oil, \\\n                    case when lower(finished_unigrams) like '%brow flick%' or lower(finished_unigrams) like '%browflick%' then 1 else 0 end as brow_flick, \\\n                    case when lower(finished_unigrams) like '%haloscope%' then 1 else 0 end as haloscope, \\\n                    case when lower(finished_unigrams) like '%lidstar%' or lower(finished_unigrams) like '%ls%' then 1 else 0 end as lidstar, \\\n                    case when lower(finished_unigrams) like '%swiss miss%' then 1 else 0 end as swiss_miss, \\\n                    case when lower(finished_unigrams) like '%le%' then 1 else 0 end as le, \\\n                    case when lower(finished_unigrams) like '%sticker%' then 1 else 0 end as sticker, \\\n                    case when lower(finished_unigrams) like '%comb%' then 1 else 0 end as comb, \\\n                    case when lower(finished_unigrams) like '%kit%' then 1 else 0 end as kit, \\\n                    case when lower(finished_unigrams) like '%body hero%' then 1 else 0 end as body_hero, \\\n                    case when lower(finished_unigrams) like '%futuredew%' then 1 else 0 end as futuredew, \\\n                    case when lower(finished_unigrams) like '%cleanser%' or lower(finished_unigrams) like '%milk jelly cleanser%' or lower(finished_unigrams) like '%jelly cleanser%' then 1 else 0 end as cleanser, \\\n                    case when lower(finished_unigrams) like '%after baume%' or lower(finished_unigrams) like '%after balm%' then 1 else 0 end as after_balm, \\\n                    case when lower(finished_unigrams) like '%moisturizer%' or lower(finished_unigrams) like '%priming moisturizer%' then 1 else 0  end as moisturizer, \\\n                    case when lower(finished_unigrams) like '%retinol%' or lower(finished_unigrams) like '%pro-retinol%' or lower(finished_unigrams) like '%universal pro-retinol%' then 1 else 0 end as retinol, \\\n                    case when lower(finished_unigrams) like '%sunscreen%' then 1 else 0 end as sunscreen, \\\n                    case when lower(finished_unigrams) like '%zit%' or lower(finished_unigrams) like '%zit sticker%' or lower(finished_unigrams) like '%zitsticker%' then 1 else 0 end as zit, \\\n                    case when lower(finished_unigrams) like '%tin%' or lower(finished_unigrams) like '%tins%' then 1 else 0 end as tin, \\\n                    case when lower(finished_unigrams) like '%cranberry%' then 1 else 0 end as cranberry, \\\n                    case when lower(finished_unigrams) like '%cordial%' then 1 else 0 end as cordial, \\\n                    case when lower(finished_unigrams) like '%olivia rodrigo%' then 1 else 0 end as olivia_rodrigo from df2_vw\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2133d356-61c0-43de-81a8-7d3baf318871","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create an NLP pipeline\n# Using the twitter sentiment pipeline because Reddit and Twitter are similar as two social media platforms\nimport sparknlp\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\n\n## Assemble the document \ndocumentAssembler = DocumentAssembler()\\\n    .setInputCol(\"text\")\\\n    .setOutputCol(\"document\")\n    \n## Use the pretrained encoder model \nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n .setInputCols([\"document\"])\\\n .setOutputCol(\"sentence_embeddings\")\n\n## Use the pretraind twitter model \nsentimentdl = SentimentDLModel.pretrained(name='sentimentdl_use_twitter', lang=\"en\")\\\n    .setInputCols([\"sentence_embeddings\"])\\\n    .setOutputCol(\"sentiment\")\n\n## Assemble and develop the pipeline \nnlpPipeline = Pipeline(\n      stages = [\n          documentAssembler,\n          use,\n          sentimentdl])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a64397c-f3b0-4464-a801-897f64f657f2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Run the sentiment analysis pipeline \nfrom pyspark.sql.functions import col\n\n## Create the dataframe \nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\npipelineModel = nlpPipeline.fit(empty_df)\n\n## Run the pipeline on the data and select the clean text column \ndata = product_df.select(col(\"finished_unigrams\").alias(\"text\"))\nresult = pipelineModel.transform(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dacec67d-1c1a-4fc8-af2e-f3684d813a94","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Only select the necessary columns using explode function \nresult = result.select('text', F.explode('sentiment.result').alias(\"sentiment\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe05e7f1-0857-4966-836d-6eb35e17ea65","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Convert the two dataframes into views to run SQL \nresult.createOrReplaceTempView(\"result_vw\")\nproduct_df.createOrReplaceTempView(\"product_df_vw\")\n\n## Combine the two dataframes in order to get the product, sentiment, and text values \ncombined_df = spark.sql(\"select product_df_vw.*, result_vw.sentiment \\\n                    from product_df_vw join result_vw on product_df_vw.finished_unigrams = result_vw.text\")\n\n## Show the first few records of the data frame \ncombined_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"57a1b935-d249-439b-bc01-37f4289294b8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Get the columns as a list of products to convert to long format\ncombined_df.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ae21cc1-1a29-4892-b699-64868c374631","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Import the necessary libraries \nfrom pyspark.sql.functions import array, col, explode, lit, struct\nfrom pyspark.sql import DataFrame\nfrom typing import Iterable\n\n## Define a function to melt the dataframe \n## Website Used: https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\ndef melt(df: DataFrame, \n        id_vars: Iterable[str], value_vars: Iterable[str], \n        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n    _vars_and_vals = array(*(\n        struct(lit(c).alias(var_name), col(c).alias(value_name)) \n        for c in value_vars))\n    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n    cols = id_vars + [\n            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n    return _tmp.select(*cols)\n\n## Call the function to convert the dataframe into long format using the column names above\nlong_df = melt(combined_df, id_vars=['body', 'finished_unigrams', 'sentiment'], value_vars=['bdc',\n 'lashslick',\n 'powder',\n 'concealer',\n 'mascara',\n 'highlighter',\n 'candle',\n 'rollerball',\n 'balm_dot_com',\n 'hoodie',\n 'tote',\n 'boybrow',\n 'brow_gel',\n 'generation_g',\n 'scarf',\n 'clip',\n 'hair_clips',\n 'perfume',\n 'cloud_paint',\n 'blush',\n 'eyeliner',\n 'pencil',\n 'skin_tint',\n 'stretch_concealer',\n 'pomade',\n 'lipstick',\n 'lash_slick',\n 'brush',\n 'eyeshadow',\n 'protip',\n 'solar_paint',\n 'bronzer',\n 'monochromes',\n 'milky_oil',\n 'brow_flick',\n 'haloscope',\n 'lidstar',\n 'swiss_miss',\n 'le',\n 'sticker',\n 'comb',\n 'kit',\n 'body_hero',\n 'futuredew',\n 'cleanser',\n 'after_balm',\n 'moisturizer',\n 'retinol',\n 'sunscreen',\n 'zit',\n 'tin',\n 'cranberry',\n 'cordial',\n 'olivia_rodrigo'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89234ba7-c841-4469-9982-9f4e60967211","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Filter the dataframe to where there is a value of 1 (product name is mentioned)\nlong_df = long_df.filter(col(\"value\") == 1)\nlong_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2f1a9f9-78de-417e-bae1-40f597a6c32e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Convert the view into a SQL \nlong_df.createOrReplaceTempView(\"long_df_vw\")\n\n## Get the count of each product \nagg_df1 = spark.sql(\"select variable,count(1) as product_count \\\n                    from long_df_vw group by 1\")\n\n## Get the count of each sentiment by product \nagg_df2 = spark.sql(\"select variable, sentiment, count(1) as total_count \\\n                    from long_df_vw group by 1,2 order by variable, count(1)\")\n\n## Rename columns to not have duplicates\nagg_df2 = agg_df2.withColumnRenamed(\"variable\",\"var\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3be7143d-e983-43ca-a742-23326f13929b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Create views of the data frames \nagg_df1.createOrReplaceTempView(\"agg_df1_vw\")\nagg_df2.createOrReplaceTempView(\"agg_df2_vw\")\n\n## Combine the two data frames \nagg_df3 = spark.sql(\"select * \\\n                    from agg_df2_vw a join agg_df1_vw b on a.var = b.variable\")\n\n## Show the dataframe \nagg_df3.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8868a0fb-6a67-4be7-8343-ef02cac6b728","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Create view of the data frame \nagg_df3.createOrReplaceTempView(\"agg_df3_vw\")\n\n## Calculate the ratio \nagg_df4 = spark.sql(\"select var, sentiment, total_count, product_count, total_count / product_count * 100 as ratio \\\n                    from agg_df3_vw\")\n\n## Rename columns to not have duplicates\nagg_df4 = agg_df4.withColumnRenamed(\"product_count\",\"total_mention\")\nagg_df4 = agg_df4.withColumnRenamed(\"total_count\",\"sentiment_count\")\n\n## Display the data frame \nagg_df4.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"61ab7043-2c76-4473-b008-815c6bd0c0c9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Filter the dataframes for positve and negative sentiment respectively\npos_df = agg_df4.filter(col(\"sentiment\") == \"positive\")\nneg_df = agg_df4.filter(col(\"sentiment\") == \"negative\")\nneg_df.createOrReplaceTempView(\"neg_df_vw\")\npos_df.createOrReplaceTempView(\"pos_df_vw\")\n\n## Get the top 10 positive products by ratio \npos_df= spark.sql(\"select * \\\n                    from pos_df_vw order by ratio desc limit 10\")\n\n## Get the top 10 negative products by ratio \nneg_df= spark.sql(\"select * \\\n                    from neg_df_vw order by ratio desc limit 10\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06d28f7e-7199-4e3f-909f-2ec7b7f73fa1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Display summary table \ncols = (\"sentiment\")\nsummary_df = pos_df.drop(cols)\nsummary_df = summary_df.withColumnRenamed(\"var\",\"product\")\nsummary_df = summary_df.withColumnRenamed(\"ratio\",\"percent_pos_mentions (%)\")\nsummary_df = summary_df.withColumnRenamed(\"sentiment_count\",\"num_positive_mentions\")\nsummary_df = summary_df.withColumnRenamed(\"total_mention\",\"num_total_mentions\")\nsummary_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b1918ed-6822-4a10-b445-b3689581f7ed","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Save the summary table to a location in repo \nimport os\nsummary_df = summary_df.toPandas()\nfpath = os.path.join(\"/Workspace/Repos/cag199@georgetown.edu/fall-2022-reddit-big-data-project-project-group-16/data/csv/\", \"top_pos_prod_sum_tbl.csv\")\nsummary_df.to_csv(fpath)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d19d8c84-b521-4bd1-917b-a00301df9f38","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Import necessary visualization libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## Convert the data frames into pandas for visualization \npos_df = pos_df.toPandas()\nneg_df = neg_df.toPandas()\n\n## Define color pallete \nclrs = [\"#9EF478\", \"#EDF197\"]\n\n## Plot the positive sentiment for each product in the glossier subredit \nsns.set(font_scale=3, rc={'figure.figsize':(20,10)})\nsns.set_theme(style='white')\np1 = sns.barplot(data=pos_df, x='var', y='ratio', hue='sentiment', palette=clrs)\np1.set_title('Positive Sentiment Ratio By Glossier Product')\np1.set_ylabel('% Positive Activity Of Total')\np1.set_xlabel('Product')\np1 = plt.xticks(rotation=45)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2138724f-6a44-4661-8a92-3a51f9926928","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Create the same visualization in plotly for interactiveness in html file \nimport plotly.express as px\nfig = px.bar(pos_df, x=\"var\", y=\"ratio\", \n             color=\"sentiment\", \n            title = \"Positive Sentiment Ratio By Glossier Product\",\n            color_discrete_sequence=[\"#9EF478\"])\n\nfig.update_layout(plot_bgcolor = \"white\",  xaxis_title=\"Product\", yaxis_title=\"% Positive Activity Of Total\", title_x=0.5,\n                 xaxis={'categoryorder':'total descending'})\nfpath = os.path.join(\"/Workspace/Repos/cag199@georgetown.edu/fall-2022-reddit-big-data-project-project-group-16/data/plots/\", \"glossier_pos_sent_viz.html\")\nnewpath = os.path.join(\"/Workspace/Repos/cag199@georgetown.edu/fall-2022-reddit-big-data-project-project-group-16/website/images/\", \"glossier_pos_sent_viz.html\")\nfig.write_html(newpath)\nfig.write_html(fpath)\nfig.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99d7d608-30b3-4963-ae23-b792a2584e0b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Define color pallete \nclrs = [\"#E3242B\"]\n\n## Plot the negative sentiment for each product in the glossier subredit \nsns.set(font_scale=3, rc={'figure.figsize':(20,10)})\nsns.set_theme(style='white')\np1 = sns.barplot(data=neg_df, x='var', y='ratio', hue='sentiment', palette=clrs)\np1.set_title('Neagtive Sentiment Ratio By Glossier Product')\np1.set_ylabel('% Negative Activity Of Total')\np1.set_xlabel('Product')\np1 = plt.xticks(rotation=45)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50d101d5-5bb2-49e4-b11a-974b980606c7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Create the same visualization in plotly for interactiveness in html file \nimport plotly.express as px\nfig = px.bar(neg_df, x=\"var\", y=\"ratio\", \n             color=\"sentiment\", \n            title = \"Negative Sentiment Ratio By Glossier Product\",\n            color_discrete_sequence=[\"#E3242B\"])\n\nfig.update_layout(plot_bgcolor = \"white\",  xaxis_title=\"Product\", yaxis_title=\"% Negative Activity Of Total\", title_x=0.5,\n                 xaxis={'categoryorder':'total descending'})\nfpath = os.path.join(\"/Workspace/Repos/cag199@georgetown.edu/fall-2022-reddit-big-data-project-project-group-16/data/plots/\", \"glossier_neg_sent_viz.html\")\nnewpath = os.path.join(\"/Workspace/Repos/cag199@georgetown.edu/fall-2022-reddit-big-data-project-project-group-16/website/images/\", \"glossier_neg_sent_viz.html\")\nfig.write_html(newpath)\nfig.write_html(fpath)\nfig.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"76ba2763-773c-4620-96ea-d6341b766771","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Filter the dataframes for neutral sentiment respectively\nneu_df = agg_df4.filter(col(\"sentiment\") == \"neutral\")\nneu_df.createOrReplaceTempView(\"neu_df_vw\")\nneu_df= spark.sql(\"select * \\\n                    from neu_df_vw order by ratio desc limit 10\")\n\n## Convert the data frames into pandas for visualization \nneu_df = neu_df.toPandas()\n\n## Define color pallete \nclrs = [\"#FAE29C\"]\n\n## Plot the negative sentiment for each product in the glossier subredit \nsns.set(font_scale=3, rc={'figure.figsize':(20,10)})\nsns.set_theme(style='white')\np1 = sns.barplot(data=neu_df, x='var', y='ratio', hue='sentiment', palette=clrs)\np1.set_title('Neutral Sentiment Ratio By Glossier Product')\np1.set_ylabel('% Neutral Activity Of Total')\np1.set_xlabel('Product')\np1 = plt.xticks(rotation=45)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35bdc0c7-a4c6-4a13-ba2a-8011324dcfdb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Create the same visualization in plotly for interactiveness in html file \nimport plotly.express as px\nfig = px.bar(neu_df, x=\"var\", y=\"ratio\", \n             color=\"sentiment\", \n            title = \"Neutral Sentiment Ratio By Glosier Product\",\n            color_discrete_sequence=[\"#FAE29C\"])\n\nfig.update_layout(plot_bgcolor = \"white\",  xaxis_title=\"Product\", yaxis_title=\"% Neutral Activity Of Total\", title_x=0.5,\n                 xaxis={'categoryorder':'total descending'})\nfpath = os.path.join(\"/Workspace/Repos/cag199@georgetown.edu/fall-2022-reddit-big-data-project-project-group-16/data/plots/\", \"glossier_neut_sent_viz.html\")\nfig.write_html(fpath)\nnewpath = os.path.join(\"/Workspace/Repos/cag199@georgetown.edu/fall-2022-reddit-big-data-project-project-group-16/website/images/\", \"glossier_neut_sent_viz.html\")\nfig.write_html(newpath)\nfig.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f19294d6-b530-40ae-bfa7-d3cfcae92dfd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab16ad9d-5d3e-48e5-9b8f-b62662de2de6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Kajal_NLP (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2760780567423597}},"nbformat":4,"nbformat_minor":0}
